import { GoogleGenerativeAI } from '@google/generative-ai'

export type ChatMessage = {
  role: 'user' | 'assistant'
  content: string
  timestamp?: Date | string
}

export type ImageFile = {
  mimeType: string
  base64Data: string
}

// Kh·ªüi t·∫°o Google Generative AI client
const getGeminiClient = () => {
  const apiKey = process.env.GEMINI_API_KEY
  if (!apiKey) {
    throw new Error('GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh')
  }
  return new GoogleGenerativeAI(apiKey)
}

// Danh s√°ch c√°c model ƒë·ªÉ th·ª≠ (theo th·ª© t·ª± ∆∞u ti√™n - t·ªëi ∆∞u nh·∫•t)
const MODELS_TO_TRY = [
  'gemini-2.0-flash-001', // üèÜ Model t·ªëi ∆∞u nh·∫•t: nhanh nh·∫•t (2218ms), ·ªïn ƒë·ªãnh, ch·∫•t l∆∞·ª£ng cao
  'gemini-2.0-flash', // Backup model 2.0 (2392ms)
  'gemini-3-27b', // Latest fallback (6337ms)
  'gemini-pro-latest' // Pro latest fallback (24165ms) - ch·∫≠m nh∆∞ng ch·∫•t l∆∞·ª£ng cao
]

const makeRequest = async (messages: ChatMessage[], subject: string, imageFile?: ImageFile): Promise<string> => {
  try {
    const genAI = getGeminiClient()

    const prompt = messages.map((msg) => msg.content).join('\n')
    // X√°c ƒë·ªãnh temperature d·ª±a tr√™n lo·∫°i task
    const getTemperature = (subject: string, prompt: string): number => {
      if (subject === 'mindmap' || prompt.includes('mindmap')) {
        return 0.3 // Th·∫•p cho JSON structure
      } else if (subject === 'review' || prompt.includes('√¥n t·∫≠p')) {
        return 0.5 // Trung b√¨nh cho educational content
      } else if (subject === 'exercise' || prompt.includes('gi·∫£i b√†i')) {
        return 0.2 // R·∫•t th·∫•p cho accuracy
      } else if (prompt.includes('GDPT 2018')) {
        return 0.4 // Th·∫•p cho curriculum compliance
      }
      return 0.7 // Default
    }

    const temperature = getTemperature(subject, prompt)

    // Th·ª≠ c√°c model theo th·ª© t·ª± ∆∞u ti√™n
    for (let i = 0; i < MODELS_TO_TRY.length; i++) {
      const modelName = MODELS_TO_TRY[i]

      try {
        const model = genAI.getGenerativeModel({
          model: modelName,
          generationConfig: {
            temperature: temperature,
            topK: 40,
            topP: 0.95,
            maxOutputTokens: 16384 // TƒÉng l√™n 16384 ƒë·ªÉ c√≥ ƒë·ªß kh√¥ng gian cho mindmap chi ti·∫øt 3 c·∫•p ƒë·ªô
          }
        })

        // T·∫°o prompt parts
        const promptParts: any[] = [{ text: prompt }]

        if (imageFile) {
          if (!imageFile.base64Data || imageFile.base64Data.trim() === '') {
            throw new Error('Image data is empty')
          }

          promptParts.push({
            inlineData: {
              mimeType: imageFile.mimeType,
              data: imageFile.base64Data
            }
          })
        }

        // S·ª≠ d·ª•ng generateContent ƒë∆°n gi·∫£n
        const result = await model.generateContent(promptParts)
        const response = await result.response
        const text = response.text()

        if (!text) {
          throw new Error('Gemini kh√¥ng tr·∫£ v·ªÅ n·ªôi dung')
        }

        // Ki·ªÉm tra xem response c√≥ b·ªã c·∫Øt c·ª•t kh√¥ng
        if (text.length < 100) {
        }

        // Ki·ªÉm tra xem c√≥ k·∫øt th√∫c ƒë·ªôt ng·ªôt kh√¥ng
        const lastWords = text.trim().split(' ').slice(-3).join(' ')
        if (lastWords.includes('**4) M·∫πo ghi') || text.endsWith('---') || text.endsWith('**4)')) {
        }

        return text
      } catch (error: any) {
        const errorMessage = error.message || error.toString()

        // N·∫øu kh√¥ng ph·∫£i model cu·ªëi c√πng, th·ª≠ model ti·∫øp theo
        if (i < MODELS_TO_TRY.length - 1) {
          continue
        } else {
          // Model cu·ªëi c√πng c≈©ng fail
          throw new Error(`T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng ho·∫°t ƒë·ªông. L·ªói cu·ªëi c√πng: ${errorMessage}`)
        }
      }
    }

    throw new Error('Kh√¥ng c√≥ model n√†o ho·∫°t ƒë·ªông')
  } catch (error: any) {
    console.error('‚ùå Gemini API error:', error.message)
    throw new Error(`Gemini API error: ${error.message}`)
  }
}

const generateText = async (prompt: string): Promise<string> => {
  return makeRequest([{ role: 'user', content: prompt }], 'text-generation')
}

export default { makeRequest, generateText }
